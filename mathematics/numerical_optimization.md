# 数值优化

在机器学习中,我们往往不能使用最优解的方式去求解问题.因为这带来的计算开销太大了.而且计算机无法通过有限的位模式来表示无限的实数域.

我们往往采用迭代算法去逼近问题的最优解,在迭代轮次达到一定程度时,得到的解可以认为就是最优解.

深度学习涉及到大量的优化,优化表示对于一个函数$f(x)$,我们改变x以最小化$f(x)$.这里的$f(x)$称为**目标函数**.在优化它的时候,也叫**误差函数**.

假设$f(x)$在某个点取得最优值,我们记这个点为$x^*$,即有:

$$x^*=argminf(x)$$

## 基于梯度的优化

对$f(x)$的优化,我们常使用其导数.导数指明了某个点在$f(x)$上的斜率.我们可以让$x$往导数的反方向移动一小步来优化$f(x)$.这种技术称为梯度下降.

我们记$x$在$f(x)$处的梯度为: $\nabla_xf(x)$.则我们更新$x$的公式为:

$$x'=x-\epsilon\nabla_xf(x)$$

其中$\epsilon$叫做**学习率**.它控制了梯度下降的速度.

梯度下降的问题在于它容易收敛于**局部最低点**.这些点的导数为0或近似为0,到这些点优化将会结束,但它不一定是**全局最低点**.对于导数为0的点,我们也叫做**临界点**.

## 二阶导数测试

我们现在关心,梯度下降的效果到底如何,怎么用数学去衡量?

一个好的方法是二阶导数,二阶导数求得导数的变化率,是对曲率的衡量.

例如,对于一个二阶函数$f$,假设$f''=0$.则这个曲线的曲率为0,它是一条平缓的直线,梯度可以正确地预测下降值.而如果$f''<0$,我们称为**负曲率**,此时函数是向下凹陷的(向上凸出),$f$实际上比梯度预测下降地更快,若$f''>0$,我们称为**正曲率**,此时函数向上凹陷(向下凸出).此时$f$比预期下降地更慢,并在后面甚至会开始增加.

如果函数是多维的,我们可以使用**Hessian矩阵**来记录它的所有偏导数,Hessian矩阵每个单元的定义为:

$$\mathbf{H}(f)(x)_{ij}=\frac{\partial^2}{\partial x_i\partial x_j}f(x)$$

对于二阶偏导,微分算子是可以交换的,这意味Hessian矩阵有一个重要的特征:

$$\mathbf{H}_{ij}=\mathbf{H} _{ji}$$

也就是说Hessian矩阵是实对称的.

在对多维函数求导的时候引入**方向导数**,方向导数是函数$f$在$u$(单位向量)方向的斜率.也就是函数$f(x+\alpha u)$关于$\alpha$的导数(在$\alpha$为0时取的),则可以计算:

$$\frac{\partial}{\partial\alpha}f(x+\alpha u)=u^T\nabla_xf(x)$$

回到Hessian矩阵,因为它是对称的,所以可以分解成一组实特征值和一组特征向量的正交基.也就是说,在特定方向$d$上的二阶导数可以写成$d^T\mathbf{H}d$.当$d$是$\mathbf{H}$的一个特征向量,则其对应的特征值就是$d$方向的二阶导数.

通过方向二阶导数可以预期一个梯度下降步骤的表现,现在假设$\mathbf{g}$是梯度,在$x^{(0)}$处对$f$在$\mathbf{g}$方向的求导可以写为$\mathbf{g}^T\mathbf{H}\mathbf{g}$,对函数做泰勒展开:

$$f(x)\approx f(x^{(0)})+(x-x^{(0)})^T\mathbf{g}+\frac{1}{2}(x-x^{(0)})^T\mathbf{H}(x-x^{(0)})$$

将梯度下降的更新公式$x^{(0)}-\epsilon\mathbf{g}$带入上面的近似,有:

$$f(x^{(0)}-\epsilon\mathbf{g})\approx f(x^{(0)})-\epsilon\mathbf{g}^T\mathbf{g}+\frac{1}{2}\epsilon^2\mathbf{g}^T\mathbf{H}\mathbf{g}$$

这个式子有三个项,第一个项是原始的函数值,第二个是预期的改善.注意第三个项,它过大会导致函数向上移动.当$\mathbf{g}^T\mathbf{H}\mathbf{g}$为负或0时,泰勒级数表明按照梯度下降将会永远使$f$下降.

在$\mathbf{g}^T\mathbf{H}\mathbf{g}$为正时,通过二次函数的计算,$\epsilon$取以下值时可以使近似泰勒级数下降得最快:

$$\epsilon^*=\frac{\mathbf{g}^T\mathbf{g}}{\mathbf{g}^T\mathbf{H}\mathbf{g}}$$

二阶导数还可以用于确定一个临界点是否是局部极大点,局部极小点:

- 当$f'=0且f''>0$时,该点是一个局部极小点.
- 当$f'=0且f''<0$时,该点是一个局部极大点.
- 当$f'=0且f''=0$时,结果不确定,该点可以是鞍点或平坦区域一部分.

在多维情况下,需要考虑函数的所有二阶导数.而$\mathbf{H}$记录了这些信息.具体来说,在临界点$\nabla_x f(x)=0$处,当$\mathbf{H}$是正定的(所有方向的二阶导数都大于0),则该临界点是局部极小点.若是负定的,则这个点是局部极大点.

在多维情况下,一个有趣的现象是,如果$\mathbf{H}$的某个特征值是正的,某个特征值是负的,那么这个点是$f$某个横截面的极大点,另外一个横截面的极小点.在三维空间中,具有上述点的函数具有"马鞍"的形状,因此上面的点也叫做"鞍点".

和二维一样,在多维情况下,只要$\mathbf{H}$有一个特征值是0,则该点的类型就是不确定的.因为特征值为0的这个方向对应的横截面就不确定了.

## 牛顿法

在多维情况下,梯度下降法往往选择"最陡峭"的方向.而最陡峭的方向不一定是函数下降得最快的方向.产生这种问题的原因就是梯度下降法没有利用函数的曲率信息.在曲率很大的方向做梯度下降是一件很蠢的事情(甚至可能产生函数上升).

**牛顿法**使用$\mathbf{H}$矩阵的信息来指导搜索.牛顿法使用一个二阶泰勒展开来近似$x^{(0)}$附近的$f(x)$:

$$f(x)\approx f(x^{(0)})+(x-x^{(0)})^T\nabla_xf(x^{(0)})+\frac{1}{2}(x-x^{(0)})^T\mathbf{H}(f)(x^{(0)})(x-x^{(0)})$$

通过计算,可以得到函数的临界点:

$$x^*=x^{(0)}-\mathbf{H}(f)(x^{(0)})^{-1}\nabla_xf(x^{(0)})$$

如果$f$是一个正定的二次函数,牛顿法只需要一次就能跳到函数的最小点.如果$f$不是一个正定二次函数而是一个局部正定的二次函数,则牛顿法可以比梯度下降法更快地逼近临界点.

在接近临界点是,牛顿法是一个很好的选择,但是它的问题是容易跳到鞍点,而梯度下降法不会跳到鞍点.

## 优化算法分类

在上面介绍的两个算法中,仅适用梯度信息优化的算法叫做**一阶优化算法**,如传统的梯度下降法,适用Hessian矩阵的优化算法叫做**二阶优化算法**,如牛顿法.

关于优化的另外一个领域是**凸优化**,凸优化通过更强的限制提供更多的保证,它限制函数是**凸函数**.对于二维来说,凸函数指的是二阶导数均大于0的函数,对于高阶函数,凸函数的Hessian矩阵半正定的.

凸函数的特点是没有鞍点并且所有局部极小点必然是全局最小点.

## 约束优化

有时候,我们并不想在$x$的所有取值下去优化一个函数,而是希望在一个集合$\mathbb{S}$中对$f$进行优化,这样的优化问题叫做**约束优化**.

一个可行的方法是,仍然使用梯度下降法,但是把结果投影到$\mathbb{S}$中.

这里重点说**Karush-Kuhn-Tucker**方法,这是约束优化问题的通用解决方法.

我们先引入一个**广义Lagrangian函数**,对于$x$的定义域$\mathbb{S}$,我们希望通过若干函数来表达.具体来说,我们希望通过$m$个函数$g^{(i)}$和$n$个函数$h^{(j)}$来描述$\mathbb{S}$.那么$\mathbb{S}$可以表示为:

$$\mathbb{S}=\{x|\forall i,g^{(i)}(x)=0\ and\ \forall j,h^{(j)}(x)\le 0 \}$$

其中,$g^{(i)}$被称为**等式约束**,$h^{(j)}$被称为**不等式约束**.

再引入$\lambda_i$和$\alpha_j$,它们被称为KKT乘子.则广义Lagrangian函数定义为:

$$L(x,\lambda,\alpha)=f(x)+\sum_i\lambda_ig^{(i)}(x)+\sum_j\alpha_jh^{(j)}(x)$$

现在,我们只需要对广义Lagrangian函数做优化,相当于求:

$$\min\limits_x\max\limits_\lambda\max\limits_{\alpha,\alpha\ge0}L(x,\lambda,\alpha)$$

这与如下函数有相同的最优目标函数值和最优点集:

$$\min\limits_{x\in\mathbb{S}}f(x)$$

这个特点非常有意思,因为当$x$满足约束时,有:

$$\max\limits_\lambda\max\limits_{\alpha,\alpha\ge0}L(x,\lambda,\alpha)=f(x)$$

当不满足的时候,有:

$$\max\limits_\lambda\max\limits_{\alpha,\alpha\ge0}L(x,\lambda,\alpha)=\infty$$

这个性质可以保证不可行点($x\notin\mathbb{S}$)永远不可能是最佳的,而可行点($x\in\mathbb{S}$)最优点不变.

为了最终解决约束最大化问题,我们可以构造$-f(x)$的广义Lagrange函数,从而得到以下优化问题:

$$\min\limits_x\max\limits_\lambda\max\limits_{\alpha,\alpha\ge0}-f(x)+\sum_i\lambda_ig^{(i)}(x)+\sum_j\alpha_jh^{(j)}(x)$$

可以转换为在外层的最大化问题:

$$\max\limits_x\min\limits_\lambda\min\limits_{\alpha,\alpha\ge0}f(x)+\sum_i\lambda_ig^{(i)}(x)-\sum_j\alpha_jh^{(j)}(x)$$

等式约束的符号并不重要,因为优化可以自由选择每个$\lambda_i$的符号.

关注不等式约束.如果一个不等式约束$h^{(i)}(x^*)=0$,则我们就说这个约束$h^{(i)}$是活跃的.如果约束不活跃,则有该约束的问题的解和去掉这个约束的问题的解至少存在一个相同的局部解.

不活跃约束可能会排除掉其他解,例如一些存在最优点的区域被不活跃约束给消去.然而,无论不活跃约束是否在内,收敛时找到的点仍然是一个临界点.因为一个不活跃约束$h^{(j)}$必有负值,那么在最小化的过程中必另$\alpha_j=0$.因此,在优化解中,有一个很有意思的现象:$\alpha\odot h(x)=0$,也就是说,要么$h(x)$为0(该约束是活跃的),要么$\alpha$为0(该约束不活跃).

换句话说,在收敛时,$\alpha_i\ge0$或$h^{(i)}(x)\le0$必有一个是活跃的.这个特性叫做**互补松弛性**.

可以使用一组简单的性质来描述约束优化问题的最优解,这些性质叫做KKT条件.这些条件是确定一个点是最优点的必要条件,但不是充分条件:

- 广义Lagrangian梯度为0
- 所有关于$x$的KKT乘子的约束都满足
- 不等式约束满足"互补松弛性": $\alpha\odot h(x)=0$.

我们可以通过KKT条件来验证求得的点是不是最优解.
