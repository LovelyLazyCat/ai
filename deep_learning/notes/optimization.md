# 深度学习中的优化笔记

深度学习中的优化问题是非常重要的.深度学习动辄就需要用到多台机器训练数天甚至数月的时间,而且仅仅是训练单个神经网络.如何设计好的优化方法,让神经网络能够更快地,更准确地找到损失函数的全局最低点已经成为了一个很重要的问题.

## 神经网络的挑战

在传统的机器学习算法中,我们一般会小心地设计目标函数和约束,使其尽量是凸的.从而避免优化问题的复杂度.然而在神经网络中,非凸情况是不可避免的.

我们来看在训练神经网络的过程中可能出现的问题.

### 病态

病态问题在凸优化中非常常见.最突出的是Hessian矩阵的病态.(详见凸优化)

在神经网络中,病态体现在梯度下降会收敛于某些情况.在使用代价函数的泰勒展开预测其梯度的时候,可以得到:

$$f(x-\epsilon g)\approx f(x)-\epsilon g^tg+\frac{1}{2}g^THg$$

当$\frac{1}{2}g^THg$超过$\epsilon g^Tg$的时候,代价函数反而增加了.也就是说,在某些情况下,梯度的范数可能会随时间增加,而不是像我们期望的那样收敛到最小.这时就产生了病态问题.此时训练将没有效果.

### 局部极小值

不像凸函数,神经网络中的代价函数可能存在着多个局部极小值点.

然而神经网络的局部极小值点并不是非凸造成的.如果一个足够大的训练集能够唯一确定一个模型的参数,那么我们称这个模型是**可辨识**的.但是对于神经网络来说,我们可以交换单元的权重而不改变模型的输出,所以神经网络是**不可辨识**的模型.

不可辨识的典型特征就是神经网络具有非常多甚至无限多个局部极小值,因为隐藏层的局部极小点会被反应在总的代价函数上.这些局部极小值都有相同的代价函数值.

如果局部极小值比全局极小值有很大的代价,那么基于梯度的优化算法会带来极大的问题.但是对于现实问题中的局部极小值,很多学者认为大部分局部极小值还是有很小的代价函数,所以是否找到真正的全局最小值并不是这么重要.重要的是找到在参数空间中代价很小的值.

### 鞍点

对于高维的非凸随机函数,鞍点出现的实际上比局部极小值出现的概率要大.在鞍点处,Hessian矩阵同时具有正负特征值.处于正特征值对应的特征向量的方向的代价更高(代价函数的函数值更大),而负特征值对应的特征向量方向的代价更低.

鞍点的某个横截面是局部极大点,另外一个横截面是局部极小点.

Hessian矩阵在局部极小点处只有正特征值(正定的),在多维情况下,Hessian矩阵正定的概率会很小,而其既有正特征又有负特征的概率明显大于其正定的概率.所以说在高维情况下局部极小点比鞍点要少很多.

但是在代价较低的时候,Hessian是正定的概率又会高了很多.因为此时已经逼近临界点了.所以我们可以根据Hessian是否正定来判断此时代价是达到于局部极小点还是鞍点.

### 梯度爆炸

高度非线性的深度网络(循环神经网络极为常见)的代价函数通常包含由多个参数连乘而导致的参数尖锐的非线性(悬崖).这些区域的导数会非常大,梯度下降将一次更新过多而导致之前做的所有优化都变成无用功.

循环神经网络的梯度爆炸问题尤为明显,所以需要引入长期记忆.

### 长期依赖

当计算图变得非常深的时候,模型可能会在持续的学习中丧失了学习到先前信息的能力,让优化变得非常困难.这个问题在循环神经网络中非常明显(循环神经网络的计算图非常深,并且共享参数).

我们从数学上感受一下,加入某个计算图中有一个反复和矩阵$W$相乘的路径,那么在$t$步之后,相当于乘了$W^t$.假设矩阵$W$的特征分解为$V\mathbf{diag}(\lambda)V^{-1}$.那么有:

$$W^t=(V\mathbf{diag}(\lambda)V^{-1})^t=V\mathbf{diag}(\lambda)^tV^{-1}$$

假如$t$非常大,则当特征值$\lambda_i$稍微比1大一些,就会出现梯度爆炸;特征值稍微比1小一些,就会出现梯度消失.

循环网络重复使用相同的参数多次,所以很容易产生长期依赖,而前馈网络因为不使用重复的参数,所以即使使用非常深的结构,也能很大程度避免长期依赖问题.

## 基本算法

### 随机梯度下降


### 动量

### Nesterov动量

## 参数化初始策略

## 自适应学习算法

### AdaGrad

### RMSProp

### Adam
